# 빅데이터는 수집, 정제, 적재, 분석, 시각화의 여러 단계를 거칩니다. 이 단계를 거치는 동안 여러가지 기술을 이용하여 처리되고, 이 기술들을 통틀어 빅데이터 에코 시스템(Bigdata Eco System)이라고 합니다.

## Hadoop ?
- 하둡은 2006년 야후의 더그 커팅이 '넛치'라는 검색엔진을 개발하는 과정에서 대용량의 비정형 데이터를 기존의 RDB 기술로는 처리가 힘들다는 것을 깨닫고, 새로운 기술을 찾는 중 구글에서 발표한 GFS와 MapReduce 관련 논문을 참고하여 개발하였습니다. 이후 아파치 재단의 오픈 소스로 공개 되었습니다. 하둡은 하나의 성능 좋은 컴퓨터를 이용하여 데이터를 처리하는 대신, 적당한 성능의 범용 컴퓨터 여러 대를 클러스터화하고, 큰 크기의 데이터를 클러스터에서 병렬로 동시에 처리하여 처리 속도를 높이는 것을 목적으로 하는 분산처리를 위한 오픈소스 프레임워크라고 할 수 있습니다.

 ## 구성 요소
- Hadoop Common
-- 하둡의 다른 모듈을 지원하기 위한 공통 컴포넌트 모듈
- Hadoop HDFS
-- 분산저장을 처리하기 위한 모듈
-- 여러개의 서버를 하나의 서버처럼 묶어서 데이터를 저장
- Hadoop YARN
-- 병렬처리를 위한 클러스터 자원관리 및 스케줄링 담당
- Hadoop Mapreduce
-- 분산되어 저장된 데이터를 병렬 처리할 수 있게 해주는 분산 처리 모듈
- Hadoop Ozone
-- 하둡을 위한 오브젝트 저장소

  ## 장단점
- 장점
-- 오픈소스로 라이선스에 대한 비용 부담이 적음
-- 시스템을 중단하지 않고, 장비의 추가가 용이(Scale Out)
-- 일부 장비에 장애가 발생하더라도 전체 시스템 사용성에 영향이 적음(Fault tolerance)
-- 저렴한 구축 비용과 비용대비 빠른 데이터 처리
-- 오프라인 배치 프로세싱에 최적화
- 단점
-- HDFS에 저장된 데이터를 변경 불가
-- 실시간 데이터 분석 같이 신속하게 처리해야 하는 작업에는 부적합
-- 너무 많은 버전과 부실한 서포트
-- 설정의 어려움
